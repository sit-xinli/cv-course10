{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "759876c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725de279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4764b4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#img_L = cv2.imread('data/tsukuba/tsukuba_1.jpg')\n",
    "#img_R = cv2.imread('data/tsukuba/tsukuba_2.jpg')\n",
    "#img_L = cv2.imread(\"data/sample_left.jpg\")  # left image\n",
    "#img_R = cv2.imread(\"data/sample_right.jpg\")  # right image\n",
    "\n",
    "img_L = cv2.imread(\"data/IMG_8301.jpg\")  # left image\n",
    "img_R = cv2.imread(\"data/IMG_8302.jpg\")  # right image\n",
    "# 必要であれば、以下の行のコメントを外して、画像サイズを変更し、より高速に処理できるようにする。\n",
    "img_L = cv2.resize(img_L, ((int)(img_L.shape[1]/4), (int)(img_L.shape[0]/4)))\n",
    "\n",
    "h,w,c = img_L.shape\n",
    "h_,w_,c_ = img_R.shape\n",
    "\n",
    "# resize img_R to match img_L\n",
    "if h_ != h or w_ != w: \n",
    "    img_R= cv2.resize(img_R, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "\n",
    "cv2.namedWindow('OriginalImage', cv2.WINDOW_AUTOSIZE)\n",
    "cv2.imshow('OriginalImage', np.hstack((img_L, img_R)))\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fea4878a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sift = cv2.SIFT_create()\n",
    "kp1, des1 = sift.detectAndCompute(img_L, None)\n",
    "kp2, des2 = sift.detectAndCompute(img_R, None)\n",
    "\n",
    "img_L_kp = cv2.drawKeypoints(img_L, kp1, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "img_R_kp = cv2.drawKeypoints(img_R, kp2, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "img_L2_kp = np.hstack((img_L_kp, img_R_kp))\n",
    "cv2.namedWindow('FeaturePoints', cv2.WINDOW_AUTOSIZE)\n",
    "cv2.imshow('FeaturePoints', img_L2_kp)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5f8cda92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bf = cv2.BFMatcher()\n",
    "matches = bf.match(des1, des2)\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "img_matches = cv2.drawMatches(img_L, kp1, img_R, kp2, matches[:200], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "cv2.namedWindow('MatchedPoints', cv2.WINDOW_AUTOSIZE)\n",
    "cv2.imshow('MatchedPoints', img_matches)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c49eb",
   "metadata": {},
   "source": [
    "### 最近隣距離比により、良いマッチングペアを選出する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "77c87535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches_rt = bf.knnMatch(des1, des2, k=2)\n",
    "good_matches = []\n",
    "for m,n in matches_rt:\n",
    "    if m.distance < 0.75*n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "good_matches = sorted(good_matches, key = lambda x:x.distance)\n",
    "img_good_matches = cv2.drawMatches(img_L, kp1, img_R, kp2, good_matches[:200], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "cv2.namedWindow('MatchedPoints200', cv2.WINDOW_AUTOSIZE)\n",
    "cv2.imshow('MatchedPoints200', img_good_matches)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b490426",
   "metadata": {},
   "source": [
    "### 最近距離比から選出した良いマッチングペアにより基礎行列Fを計算する。算出したFにより外れポイントを外す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2f4e4490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pts_L = []\n",
    "pts_R = []\n",
    "\n",
    "for i,m in enumerate(good_matches):\n",
    "    pts_L.append(kp1[m.queryIdx].pt)\n",
    "    pts_R.append(kp2[m.trainIdx].pt)\n",
    "\n",
    "import numpy as np\n",
    "pts_L = np.int32(pts_L)\n",
    "pts_R = np.int32(pts_R)\n",
    "\n",
    "# RANSACを用いて基礎行列を求める、同時に基礎行列にあったインライアを抽出する\n",
    "F, mask = cv2.findFundamentalMat(pts_L, pts_R, cv2.RANSAC)\n",
    "\n",
    "# 基礎行列を用いてエピポーラ線を描画する\n",
    "pts_L_inliers = pts_L[mask.ravel() == 1]\n",
    "pts_R_inliers = pts_R[mask.ravel() == 1]\n",
    "good_matches_inliers = [m for i,m in enumerate(good_matches[:200]) if mask[i,0] == 1]\n",
    "img_good_matches_inliers = cv2.drawMatches(img_L,kp1,img_R,kp2,good_matches_inliers[:200],None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "cv2.namedWindow('GoodMatchedPoints', cv2.WINDOW_AUTOSIZE)\n",
    "cv2.imshow('GoodMatchedPoints', img_good_matches_inliers)\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9d203d",
   "metadata": {},
   "source": [
    "### エピポーラ線を描画する。\n",
    "- エピポーラ線 ax+by+c=0の(a,b,c)は(r[0],r[1],[r[2]])\n",
    "- エピポーラ線の両端はx=0の時とx=画像幅の時"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e1930af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# エピポーラ線を描画する関数\n",
    "def drawlines(img1,img2,lines, pts1,pts2):\n",
    "    ''' img1 - image on which we draw the epilines for the points in img2\n",
    "        lines - corresponding epilines '''\n",
    "    _,w,ch = img1.shape\n",
    "    for r,pt1,pt2 in zip(lines,pts1,pts2):\n",
    "        color = tuple(np.random.randint(0,255,ch).tolist())\n",
    "        x0,y0 = map(int, [0, -r[2]/r[1] ])\n",
    "        x1,y1 = map(int, [w, -(r[2]+r[0]*w)/r[1] ])\n",
    "        img1 = cv2.line(img1, (x0,y0), (x1,y1), color,1) # draw epilines in img1\n",
    "        img1 = cv2.circle(img1,tuple(pt1),5,color,-1) # draw epilines in img1\n",
    "        img2 = cv2.circle(img2,tuple(pt2),5,color,-1) # draw epilines in img2\n",
    "    return img1,img2\n",
    "\n",
    "#右画像の特徴点のエピポーラ線は左の画像に描画する\n",
    "#pts_R_inliers_200 = pts_R_inliers[0:200,:]\n",
    "lines_L = cv2.computeCorrespondEpilines(pts_R_inliers.reshape(-1,1,2),2,F)\n",
    "lines_L = lines_L.reshape(-1,3)\n",
    "img_L_with_epipole_lines,img_R_with_sift_points = drawlines(img_L.copy(),img_R.copy(),lines_L,pts_L,pts_R)\n",
    "\n",
    "\n",
    "#左画像の特徴点のエピポーラ線は右の画像に描画する\n",
    "#pts_L_inliers_200 = pts_L_inliers[0:200,:]\n",
    "lines_R = cv2.computeCorrespondEpilines(pts_L_inliers.reshape(-1,1,2),1,F)\n",
    "lines_R = lines_R.reshape(-1,3)\n",
    "img_R_with_epipole_lines,img_L_with_sift_points = drawlines(img_R.copy(),img_L.copy(),lines_R,pts_R,pts_L)\n",
    "img_with_epipole_lines = np.hstack((img_L_with_epipole_lines,img_R_with_epipole_lines))\n",
    "cv2.namedWindow('EpipoleLines', cv2.WINDOW_AUTOSIZE)\n",
    "cv2.imshow('EpipoleLines', img_with_epipole_lines)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baab0ff",
   "metadata": {},
   "source": [
    "### さらに、Fで外れ値を削除した後の200個のペアを選んでにより基礎行列Fを計算して、エピポーラ線を可視化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "878b2ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下記のコードはH1,H2が不安定\n",
    "#pts_L = pts_L_inliers\n",
    "#pts_R = pts_R_inliers\n",
    "\n",
    "# SIFTの良いペア点を使ったら、安定性が向上。この点数を調整してください!!!\n",
    "matchingNUM = 200  # SIFTの良いペア点を使う\n",
    "pts_L = pts_L[0:matchingNUM,:]\n",
    "pts_R = pts_R[0:matchingNUM,:]\n",
    "\n",
    "F, mask = cv2.findFundamentalMat(pts_L, pts_R, cv2.RANSAC)\n",
    "\n",
    "pts_L_inliers = pts_L[mask.ravel() == 1]\n",
    "pts_R_inliers = pts_R[mask.ravel() == 1]\n",
    "\n",
    "#右画像の特徴点のエピポーラ線は左の画像に描画する\n",
    "lines_L = cv2.computeCorrespondEpilines(pts_R_inliers.reshape(-1,1,2),2,F)\n",
    "lines_L = lines_L.reshape(-1,3)\n",
    "img_L_with_epipole_lines,img_R_with_sift_points = drawlines(img_L.copy(),img_R.copy(),lines_L,pts_L,pts_R)\n",
    "\n",
    "#左画像の特徴点のエピポーラ線は右の画像に描画する\n",
    "lines_R = cv2.computeCorrespondEpilines(pts_L_inliers.reshape(-1,1,2),1,F)\n",
    "lines_R = lines_R.reshape(-1,3)\n",
    "img_R_with_epipole_lines,img_L_with_sift_points = drawlines(img_R.copy(),img_L.copy(),lines_R,pts_R,pts_L)\n",
    "img_with_epipole_lines = np.hstack((img_L_with_epipole_lines,img_R_with_epipole_lines))\n",
    "cv2.namedWindow('GoodEpipoleLines', cv2.WINDOW_AUTOSIZE)\n",
    "cv2.imshow('GoodEpipoleLines', img_with_epipole_lines)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bf300a",
   "metadata": {},
   "source": [
    "## F から整列するホモグラフィ変換Ｈ１，Ｈ２を得て、整列後の画像枠を計算する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "97ac3e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h, w ,c= img_L.shape\n",
    "retval, H1, H2 = cv2.stereoRectifyUncalibrated(\n",
    "    np.float32(pts_L_inliers),\n",
    "    np.float32(pts_R_inliers),\n",
    "    F, imgSize=(w, h), threshold=0\n",
    ")\n",
    "\n",
    "imgL_rectify = cv2.warpPerspective(img_L, H1, (w, h))\n",
    "imgR_rectify = cv2.warpPerspective(img_R, H2, (w, h))\n",
    "\n",
    "cv2.namedWindow('RectifiedImage', cv2.WINDOW_AUTOSIZE)\n",
    "cv2.imshow('RectifiedImage', np.hstack((imgL_rectify, imgR_rectify)))\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e19788",
   "metadata": {},
   "source": [
    "## 奥行きマップ推定の前処理で精度向上\n",
    "- カラーからグレイ画像へ変換\n",
    "- ノイズ除去\n",
    "- 局所ヒストグラム均等化\n",
    "- 露出の正規化\n",
    "- 明るすぎる領域の検出と補正\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "49a0777d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_stereo_images(left_img, right_img):\n",
    "    def preprocess(img):\n",
    "        # グレースケール変換\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # ガウシアンブラーでノイズ除去\n",
    "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "        # CLAHE（局所ヒストグラム均等化）でコントラスト強調（clipLimitを低めに）\n",
    "        clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8, 8))\n",
    "        clahe_img = clahe.apply(blurred)\n",
    "\n",
    "        # 露出の正規化（明るさを均一化）\n",
    "        normalized = cv2.normalize(clahe_img, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "\n",
    "        # ハイライト抑制：明るすぎる領域を検出して補正\n",
    "        highlight_mask = normalized > 240\n",
    "        suppressed = normalized.copy()\n",
    "        suppressed[highlight_mask] = cv2.medianBlur(normalized, 5)[highlight_mask]\n",
    "\n",
    "        return suppressed\n",
    "\n",
    "    # 左右画像に前処理を適用\n",
    "    left_processed = preprocess(left_img)\n",
    "    right_processed = preprocess(right_img)\n",
    "\n",
    "    return left_processed, right_processed\n",
    "\n",
    "\n",
    "# 前処理を適用\n",
    "imgL_preprocessed, imgR_preprocessed = preprocess_stereo_images(imgL_rectify, imgR_rectify)\n",
    "cv2.namedWindow('PreProcessedImage', cv2.WINDOW_AUTOSIZE)\n",
    "cv2.imshow('PreProcessedImage', np.hstack((imgL_preprocessed, imgR_preprocessed)))\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c678426",
   "metadata": {},
   "source": [
    "## ステレオマッチングでStereoデプス推定\n",
    "\n",
    "| パラメータ名         | 説明                                                                 | 調整のポイント・推奨値例                     |\n",
    "|----------------------|----------------------------------------------------------------------|----------------------------------------------|\n",
    "| `minDisparity`       | 最小視差。通常は0。                                                  | カメラの配置によっては負の値も可能。         |\n",
    "| `numDisparities`     | 視差の範囲。16の倍数で指定。                                         | 例：64, 128。対象物の距離に応じて調整。      |\n",
    "| `blockSize`          | マッチングに使うブロックサイズ（奇数）。                            | 小さいほど詳細、ただしノイズに弱くなる。     |\n",
    "| `P1`                 | 小さな視差変化へのペナルティ。滑らかさの制御。                      | `8 * n_channels * blockSize^2` が目安。       |\n",
    "| `P2`                 | 大きな視差変化へのペナルティ。滑らかさの強さを決定。                | `P1` の4倍以上が推奨。                        |\n",
    "| `disp12MaxDiff`      | 左右の視差マップの差の許容値。                                       | 小さいほど厳密。例：9。                       |\n",
    "| `uniquenessRatio`    | 一意性のしきい値（%）。                                              | 高いほどノイズ除去に有効。例：10〜15。       |\n",
    "| `speckleWindowSize`  | スペックル除去のウィンドウサイズ。                                  | 小さなノイズ領域を除去。例：50〜200。        |\n",
    "| `speckleRange`       | スペックル除去の視差範囲。                                           | 視差の変化がこの範囲内ならノイズとみなす。   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "88220111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. SGBMステレオマッチング\n",
    "min_disp = -4   # 最小視差\n",
    "max_disp = 12 + 48  # 最大視差\n",
    "num_disp = max_disp-min_disp  # 16の倍数\n",
    "block_size = 5 # ブロックサイズ（マッチングウィンドウのサイズ）\n",
    "\n",
    "stereo = cv2.StereoSGBM_create(\n",
    "    minDisparity=min_disp, # 最小視差\n",
    "    numDisparities=num_disp, # 視差の範囲\n",
    "    blockSize=block_size, # ブロックサイズ\n",
    "    # P1とP2は調整可能なパラメータ\n",
    "    # P1は小さな視差のペナルティ、P2は大きな視差のペナルティ\n",
    "    P1=8 * c * block_size ** 2, \n",
    "    P2=64 * c * block_size ** 2,\n",
    "    disp12MaxDiff=9, # 左右の視差の最大差\n",
    "    uniquenessRatio=1, # 一意性比\n",
    "    speckleWindowSize=100, # スペックルウィンドウサイズ\n",
    "    speckleRange=16 # スペックル範囲\n",
    ")\n",
    "disparity = stereo.compute(imgL_preprocessed, imgR_preprocessed).astype(np.float32) / 16.0\n",
    "disparity = cv2.normalize(disparity, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "# extend disparity to 3 channels for visualization\n",
    "disparity_c3 = cv2.cvtColor(disparity.astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "img_output = np.hstack((img_L,disparity_c3, img_R))\n",
    "cv2.namedWindow('DisparityMap(SGBM)', cv2.WINDOW_AUTOSIZE)\n",
    "cv2.imshow(\"DisparityMap(SGBM)\", img_output)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# 画像を保存\n",
    "cv2.imwrite('data/image_disparity.jpg', img_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-course10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
